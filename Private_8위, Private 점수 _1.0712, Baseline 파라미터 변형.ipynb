{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"private_8위.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"n6yF_jFypKwH"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xt4xJZ9ipDvT"},"source":["import pandas as pd\n","import numpy as np\n","import os\n","import shutil\n","from tqdm import tqdm\n","from glob import glob\n","import librosa\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9_eY-BWRpDvX"},"source":["### 데이터 불러오기"]},{"cell_type":"code","metadata":{"id":"Bm1Cw0gtpPW6"},"source":["path = '/content/drive/MyDrive/voice/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zaT2yX2mpDvY"},"source":["sample_submission = pd.read_csv(path+\"sample_submission.csv\")\n","\n","africa_train_paths = glob(path+\"train/africa/*.wav\")\n","australia_train_paths = glob(path+\"train/australia/*.wav\")\n","canada_train_paths = glob(path+\"train/canada/*.wav\")\n","england_train_paths = glob(path+\"train/england/*.wav\")\n","hongkong_train_paths = glob(path+\"train/hongkong/*.wav\")\n","us_train_paths = glob(path+\"train/us/*.wav\")\n","\n","path_list = [africa_train_paths, australia_train_paths, canada_train_paths,\n","             england_train_paths, hongkong_train_paths, us_train_paths]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_89m5RVpDvY","outputId":"c4454217-e400-4473-fd74-2da3fd74f8cb"},"source":["# glob로 test data의 path를 불러올때 순서대로 로드되지 않을 경우를 주의해야 합니다.\n","# test_ 데이터 프레임을 만들어서 나중에 sample_submission과 id를 기준으로 merge시킬 준비를 합니다.\n","\n","def get_id(data):\n","    return np.int(data.split(\"/\")[-1].split(\".\")[-2])\n","\n","test_ = pd.DataFrame(index = range(0, 6100), columns = [\"path\", \"id\"])\n","test_[\"path\"] = glob(path+\"test/*.wav\")\n","test_[\"id\"] = test_[\"path\"].apply(lambda x : get_id(x))\n","\n","test_.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>path</th>\n","      <th>id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>./open/test\\1.wav</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>./open/test\\10.wav</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>./open/test\\100.wav</td>\n","      <td>100</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>./open/test\\1000.wav</td>\n","      <td>1000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>./open/test\\1001.wav</td>\n","      <td>1001</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   path    id\n","0     ./open/test\\1.wav     1\n","1    ./open/test\\10.wav    10\n","2   ./open/test\\100.wav   100\n","3  ./open/test\\1000.wav  1000\n","4  ./open/test\\1001.wav  1001"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"lG4LptorpfEQ"},"source":["#test_.to_csv(path+\"test_.csv\", index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AXPiMNRfpDva"},"source":["### 데이터 전처리"]},{"cell_type":"markdown","metadata":{"id":"aw-fnLO3pDva"},"source":["baseline 코드에서는 librosa 라이브러리를 사용하여 wav파일을 전처리 합니다."]},{"cell_type":"code","metadata":{"id":"7VSYfk9jpDvb"},"source":["def load_data(paths):\n","\n","    result = []\n","    for path in tqdm(paths):\n","        # sr = 16000이 의미하는 것은 1초당 16000개의 데이터를 샘플링 한다는 것입니다.\n","        data, sr = librosa.load(path, sr = 16000)\n","        result.append(data)\n","    result = np.array(result) \n","    # 메모리가 부족할 때는 데이터 타입을 변경해 주세요 ex) np.array(data, dtype = np.float32)\n","\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uxWJPSfrpDvc"},"source":["# train 데이터를 로드하기 위해서는 많은 시간이 소모 됩니다.\n","# 따라서 추출된 정보를 npy파일로 저장하여 필요 할 때마다 불러올 수 있게 준비합니다.\n","\"\"\"\n","os.mkdir(\"./npy_data\")\n","\n","africa_train_data = load_data(africa_train_paths)\n","np.save(\"./npy_data/africa_npy\", africa_train_data)\n","\n","australia_train_data = load_data(australia_train_paths)\n","np.save(\"./npy_data/australia_npy\", australia_train_data)\n","\n","canada_train_data = load_data(canada_train_paths)\n","np.save(\"./npy_data/canada_npy\", canada_train_data)\n","\n","england_train_data = load_data(england_train_paths)\n","np.save(\"./npy_data/england_npy\", england_train_data)\n","\n","hongkong_train_data = load_data(hongkong_train_paths)\n","np.save(\"./npy_data/hongkong_npy\", hongkong_train_data)\n","\n","us_train_data = load_data(us_train_paths)\n","np.save(\"./npy_data/us_npy\", us_train_data)\n","\n","test_data = load_data(test_[\"path\"])\n","np.save(\"./npy_data/test_npy\", test_data)\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7oYQwKcnpDvd"},"source":["# npy파일로 저장된 데이터를 불러옵니다.\n","\"\"\"\n","africa_train_data = np.load(\"./npy_data/africa_npy.npy\", allow_pickle = True)\n","australia_train_data = np.load(\"./npy_data/australia_npy.npy\", allow_pickle = True)\n","canada_train_data = np.load(\"./npy_data/canada_npy.npy\", allow_pickle = True)\n","england_train_data = np.load(\"./npy_data/england_npy.npy\", allow_pickle = True)\n","hongkong_train_data = np.load(\"./npy_data/hongkong_npy.npy\", allow_pickle = True)\n","us_train_data = np.load(\"./npy_data/us_npy.npy\", allow_pickle = True)\n","\n","test_data = np.load(\"./npy_data/test_npy.npy\", allow_pickle = True)\n","\n","train_data_list = [africa_train_data, australia_train_data, canada_train_data, england_train_data, hongkong_train_data, us_train_data]\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IOUVIy5JprD8"},"source":["npy 파일을 따로 저장시킴"]},{"cell_type":"code","metadata":{"id":"b794sGlopDvd"},"source":["# 이번 대회에서 음성은 각각 다른 길이를 갖고 있습니다.\n","# baseline 코드에서는 음성 중 길이가 가장 작은 길이의 데이터를 기준으로 데이터를 잘라서 사용합니다.\n","\n","def get_mini(data):\n","\n","    mini = 9999999\n","    for i in data:\n","        if len(i) < mini:\n","            mini = len(i)\n","\n","    return mini\n","\n","#음성들의 길이를 맞춰줍니다.\n","\n","def set_length(data, d_mini):\n","\n","    result = []\n","    for i in data:\n","        result.append(i[:d_mini])\n","    result = np.array(result)\n","\n","    return result\n","\n","#feature를 생성합니다.\n","\n","def get_feature(data, sr = 16000, n_fft = 256, win_length = 200, hop_length = 160, n_mels = 64):\n","    mel = []\n","    for i in data:\n","        # win_length 는 음성을 작은 조각으로 자를때 작은 조각의 크기입니다.\n","        # hop_length 는 음성을 작은 조각으로 자를때 자르는 간격을 의미합니다.\n","        # n_mels 는 적용할 mel filter의 개수입니다.\n","        mel_ = librosa.feature.melspectrogram(i, sr = sr, n_fft = n_fft, win_length = win_length, hop_length = hop_length, n_mels = n_mels)\n","        mel.append(mel_)\n","    mel = np.array(mel)\n","    mel = librosa.power_to_db(mel, ref = np.max)\n","\n","    mel_mean = mel.mean()\n","    mel_std = mel.std()\n","    mel = (mel - mel_mean) / mel_std\n","\n","    return mel"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5zde0lSgpDve"},"source":["train_x = np.concatenate(train_data_list, axis= 0)\n","test_x = np.array(test_data)\n","\n","# 음성의 길이 중 가장 작은 길이를 구합니다.\n","\n","train_mini = get_mini(train_x)\n","test_mini = get_mini(test_x)\n","\n","mini = np.min([train_mini, test_mini])\n","\n","# data의 길이를 가장 작은 길이에 맞춰 잘라줍니다.\n","\n","train_x = set_length(train_x, mini)\n","test_x = set_length(test_x, mini)\n","\n","# librosa를 이용해 feature를 추출합니다.\n","\n","train_x = get_feature(data = train_x)\n","test_x = get_feature(data = test_x)\n","\n","train_x = train_x.reshape(-1, train_x.shape[1], train_x.shape[2], 1)\n","test_x = test_x.reshape(-1, test_x.shape[1], test_x.shape[2], 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wNy1g7b0pDve"},"source":["# train_data의 label을 생성해 줍니다.\n","\n","train_y = np.concatenate((np.zeros(len(africa_train_data), dtype = np.int),\n","                        np.ones(len(australia_train_data), dtype = np.int),\n","                         np.ones(len(canada_train_data), dtype = np.int) * 2,\n","                         np.ones(len(england_train_data), dtype = np.int) * 3,\n","                         np.ones(len(hongkong_train_data), dtype = np.int) * 4,\n","                         np.ones(len(us_train_data), dtype = np.int) * 5), axis = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ujMV3vEpDvf","outputId":"fe878611-7ec9-441e-d433-a019d7e1aa12"},"source":["train_x.shape, train_y.shape, test_x.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((25520, 64, 501, 1), (25520,), (6100, 64, 501, 1))"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"A823_STppDvf"},"source":["### 분석 모델"]},{"cell_type":"code","metadata":{"id":"sV-WNeK5pDvg"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import (Input, Convolution2D, BatchNormalization, Flatten,\n","                                     Dropout, Dense, AveragePooling2D, Add)\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o97JKb23pDvg"},"source":["def block(input_, units = 32, dropout_rate = 0.3):\n","    \n","    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(input_)\n","    x = BatchNormalization()(x)\n","    x_res = x\n","    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n","    x = BatchNormalization()(x)\n","    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n","    x = BatchNormalization()(x)\n","    x = Add()([x, x_res])\n","    x = AveragePooling2D()(x)\n","    x = Dropout(rate=dropout_rate)(x)\n","    \n","    return x\n","\n","def second_block(input_, units = 64, dropout_rate = 0.3):\n","    \n","    x = Convolution2D(units, 1, padding =\"same\", activation = \"relu\")(input_)\n","    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n","    x = Convolution2D(units * 4, 1, padding =\"same\", activation = \"relu\")(x)\n","    x = BatchNormalization()(x)\n","    x_res = x\n","    x = Convolution2D(units, 1, padding =\"same\", activation = \"relu\")(x)\n","    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n","    x = Convolution2D(units * 4, 1, padding =\"same\", activation = \"relu\")(x)\n","    x = BatchNormalization()(x)\n","    x = Convolution2D(units, 1, padding = \"same\", activation = \"relu\")(x)\n","    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n","    x = Convolution2D(units * 4, 1, padding = \"same\", activation = \"relu\")(x)\n","    x = BatchNormalization()(x)\n","    x = Add()([x, x_res])\n","    x = AveragePooling2D()(x)\n","    x = Dropout(rate=dropout_rate)(x)\n","    \n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ev4Mru8TpDvg"},"source":["def build_fn():\n","    dropout_rate = 0.3\n","    \n","    in_ = Input(shape = (train_x.shape[1:]))\n","    \n","    block_01 = block(in_, units = 32, dropout_rate = dropout_rate)\n","    block_02 = block(block_01, units = 64, dropout_rate = dropout_rate)\n","    block_03 = block(block_02, units = 128, dropout_rate = dropout_rate)\n","\n","    block_04 = second_block(block_03, units = 64, dropout_rate = dropout_rate)\n","    block_05 = second_block(block_04, units = 128, dropout_rate = dropout_rate)\n","\n","    x = Flatten()(block_05)\n","\n","    x = Dense(units = 128, activation = \"relu\")(x)\n","    x = BatchNormalization()(x)\n","    x_res = x\n","    x = Dropout(rate = dropout_rate)(x)\n","\n","    x = Dense(units = 128, activation = \"relu\")(x)\n","    x = BatchNormalization()(x)\n","    x = Add()([x_res, x])\n","    x = Dropout(rate = dropout_rate)(x)\n","\n","    model_out = Dense(units = 6, activation = 'softmax')(x)\n","    model = Model(in_, model_out)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nI6MU5IepDvh"},"source":["### 모델 학습"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"yMFs5vy-pDvh"},"source":["split = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 10)\n","\n","pred = []\n","pred_ = []\n","\n","for train_idx, val_idx in split.split(train_x, train_y):\n","    x_train, y_train = train_x[train_idx], train_y[train_idx]\n","    x_val, y_val = train_x[val_idx], train_y[val_idx]\n","\n","    model = build_fn()\n","    model.compile(optimizer = keras.optimizers.Adam(0.002),\n","                 loss = keras.losses.SparseCategoricalCrossentropy(),\n","                 metrics = ['acc'])\n","\n","    history = model.fit(x = x_train, y = y_train, validation_data = (x_val, y_val), epochs = 15)\n","    print(\"*******************************************************************\")\n","    pred.append(model.predict(test_x))\n","    pred_.append(np.argmax(model.predict(test_x), axis = 1))\n","    print(\"*******************************************************************\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1z4DLLGupDvh"},"source":["### 예측하기"]},{"cell_type":"code","metadata":{"id":"9Pta8zXVpDvh"},"source":["def cov_type(data):\n","    return np.int(data)\n","\n","# 처음에 살펴본 것처럼 glob로 test data의 path는 sample_submission의 id와 같이 1,2,3,4,5.....으로 정렬 되어있지 않습니다.\n","# 만들어둔 test_ 데이터프레임을 이용하여 sample_submission과 predict값의 id를 맞춰줍니다.\n","\n","result = pd.concat([test_, pd.DataFrame(np.mean(pred, axis = 0))], axis = 1).iloc[:, 1:]\n","result[\"id\"] = result[\"id\"].apply(lambda x : cov_type(x))\n","\n","result = pd.merge(sample_submission[\"id\"], result)\n","result.columns = sample_submission.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"POxNr2u1pDvh"},"source":["result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bvcFzMNTpDvi"},"source":["result.to_csv(path+\"baseline.csv\", index = False)"],"execution_count":null,"outputs":[]}]}